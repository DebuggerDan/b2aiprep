{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from b2aiprep.dataset import VBAIDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reformatted the data into a [BIDS](https://bids-standard.github.io/bids-starter-kit/folders_and_files/folders.html)-like format. The data is stored in a directory with the following structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "    sub-01/\n",
    "        ses-01/\n",
    "            beh/\n",
    "                sub-01_ses-01_questionnaire.json\n",
    "    sub-02/\n",
    "        ses-01/\n",
    "            beh/\n",
    "                sub-01_ses-01_questionnaire.json\n",
    "    ...\n",
    "```\n",
    "\n",
    "i.e. the data reflects a subject-session-datatype hierarchy. The `beh` subfolder, for behavioural data, was chosen to store questionnaire data as it is the closest approximation to the data collected.\n",
    "\n",
    "We have provided utilities which load in data from the BIDS-like dataset structure. The only input needed is the folder path which stores the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: allow user to specify input folder input\n",
    "dataset = VBAIDataset('../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every user has a sessionschema which we can get info for the users from\n",
    "qs = dataset.load_questionnaires('sessionschema')\n",
    "q_dfs = []\n",
    "for i, questionnaire in enumerate(qs):\n",
    "    # get the dataframe for this questionnaire\n",
    "    df = dataset.questionnaire_to_dataframe(questionnaire)\n",
    "    df['dataframe_number'] = i\n",
    "    q_dfs.append(df)\n",
    "    i += 1\n",
    "\n",
    "# concatenate all the dataframes\n",
    "sessionschema_df = pd.concat(q_dfs)\n",
    "sessionschema_df = pd.pivot(sessionschema_df, index='dataframe_number', columns='linkId', values='valueString')\n",
    "sessionschema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above session schema gives us some information about each \"session\" which occurred for each participant. In this context, a session is a clinical encounter where the participant answers a number of questions and has a number of audio recordings taken. The above dataframe has\n",
    "\n",
    "- `record_id` - a unique identifier for the individual\n",
    "- `session_duration` - the length in seconds of the session\n",
    "- `session_id` - a unique identifier for the session\n",
    "- `session_is_control_participant` - \"Yes\" or \"No\" indicating whether the participant is a control\n",
    "- `session_site` - The site of data collection: one of \"Mt. Sinai\", \"VUMC\", \"USF\", \"WCM\", or \"MIT\"\n",
    "- `session_status` - This indicates if the session was completed. Since we are using the deidentified release data, every row should be \"Completed\".\n",
    "\n",
    "Note what happened in the above cell to get us this `sessionschema_df` dataframe:\n",
    "\n",
    "1. We determined the name of the QuestionnaireResponse that we wanted to load in. In this case it was `'sessionschema'`.\n",
    "2. We called `load_questionnaires(questionnare_name)` with the name of the questionnaire. This function returned a dictionary: the keys are the participant identifiers (`record_id`), and the values were a set of `QuestionnaireResponse` objects which matched the questionnaire name.\n",
    "3. We then iterated through each participant's `QuestionnaireResponse` objects and converted them to pandas dataframes using `questionnaire_to_dataframe()`.\n",
    "4. We concatenated the resultant dataframes together.\n",
    "5. The concatenated dataframe was in \"long\" form, with three columns: the participant identifier (`record_id`), a unique identifier for each question (`linkId`), and the response given (`valueString`). We used `pd.pivot()` in order to transform this into a \"wide\" dataframe, with one column per question.\n",
    "\n",
    "The above process is involved, but consistent across questionnaires. For convenience, the `load_and_pivot_questionnaire` helper function automatically performs these tasks for a given questionnaire. Let's try it with the demographics dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = dataset.load_and_pivot_questionnaire('qgenericdemographicsschema')\n",
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through a couple of columns and summarize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['children', 'country', 'ethnicity', 'gender_identity', 'grandparent', 'housing_status']:\n",
    "    print(demographics_df[column].value_counts(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding data\n",
    "\n",
    "The dataset functions are convenient, but they require some prior knowledge: we needed to know that `sessionschema` was the name for the questionnaire with session information. We also needed to know that `demographics` was the name for the questionnaire where general demographics were collected. For convenience, the dataset object has another method which tells you all of the questionnaire names available. It accomplishes this by iterating through every file of the BIDS dataset. Note that this can be an expensive operation! Luckily, if there are less than 10,000 files, it goes pretty fast. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_types = dataset.list_questionnaire_types()\n",
    "for q in questionnaire_types:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above lists out the possible inputs to questionnaire types. Note that some questionnaires are only asked once per subject, such as eligibility. We can limit our list to only these questionnaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_types = dataset.list_questionnaire_types(subject_only=True)\n",
    "for q in questionnaire_types:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that `eligibility`, `enrollment`, `participant` and a number of disease specific questionnaires are only ever asked once for the participant. As before, we can use `load_and_pivot_questionnaire()` to load in this data into a convenient to use dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eligibility_df = dataset.load_and_pivot_questionnaire('eligibility')\n",
    "eligibility_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participants dataframe\n",
    "\n",
    "We can get a dataframe summarizing the participants in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_df = dataset.load_and_pivot_questionnaire('participant')\n",
    "participant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of participant by enrollment institution\n",
    "plt.figure(figsize=(10, 5))\n",
    "participant_df['enrollment_institution'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session data\n",
    "\n",
    "Load in the `QuestionnaireResponse` objects for the session schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_schema = dataset.load_questionnaires('sessionschema')\n",
    "\n",
    "# Each element is a QuestionnaireResponse, a pydantic object\n",
    "# you can serialize it to a python dictionary with .dict()\n",
    "# and to a json with .json()\n",
    "# otherwise attributes are accessible like any other python object\n",
    "questionnaire = session_schema[0]\n",
    "\n",
    "print(f'FHIR ID: {questionnaire.id}')\n",
    "print(f'First item response: {questionnaire.item[0]}')\n",
    "print('\\nAbridged questionnaire as JSON:')\n",
    "# only print the first 600 characters of the JSON for brevity\n",
    "print(questionnaire.json(indent=2)[:660], end='\\n...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which loads in questionnaires ending with a specific schema name as a dataframe\n",
    "session_df = dataset.load_and_pivot_questionnaire('sessionschema')\n",
    "session_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific questionnaire which is collected for each session in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_confounders = dataset.load_questionnaires('qgenericconfoundersschema')\n",
    "questionnaire = session_confounders[0]\n",
    "\n",
    "# Each element is a QuestionnaireResponse, a pydantic object\n",
    "# you can serialize it to a python dictionary with .dict()\n",
    "# and to a json with .json()\n",
    "# otherwise attributes are accessible like any other python object\n",
    "print(questionnaire.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic tasks\n",
    "\n",
    "Let's look at the acoustic tasks now. Acoustic task files are organized in the following way:\n",
    "\n",
    "```\n",
    "data/\n",
    "    sub-01/\n",
    "        ses-01/\n",
    "            beh/\n",
    "                sub-01_ses-01_task-<TaskName>_acoustictaskschema.json\n",
    "                sub-01_ses-01_task-<TaskName>_rec-<TaskName>-1_recordingschema.json\n",
    "                ...\n",
    "```\n",
    "\n",
    "where `TaskName` is the name of the acoustic task, including:\n",
    "\n",
    "* `Audio-Check`\n",
    "* `Cinderalla-Story`\n",
    "* `Rainbow-Passage`\n",
    "\n",
    "etc. The audio tasks are listed currently in b2aiprep/prepare.py:_AUDIO_TASKS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks = dataset.load_questionnaires('acoustictaskschema')\n",
    "print(acoustic_tasks[0].json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the above corresponds to a different acoustic task: an audio check, prolonged vowels, etc. The `value_counts()` method for pandas DataFrames lets us count all the unique values for a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df = dataset.load_and_pivot_questionnaire('acoustictaskschema')\n",
    "acoustic_tasks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above will list out all of the acoustic tasks, as every acoustic task is associated with a single \"acoustictaskschema\" `QuestionnaireResponse` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_tasks_df['acoustic_task_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from b2aiprep.process import Audio, specgram\n",
    "import IPython.display as Ipd\n",
    "\n",
    "# configuration options\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "\n",
    "base_path = Path('/Users/alistairewj/git/b2aiprep/output/')\n",
    "audio_file = base_path.joinpath(\n",
    "    'sub-1f9475bb-f13b-4f68-969b-28f20455b3e7',\n",
    "    'ses-CB8A74EE-0C8C-4B15-B322-D93A79ADB40A',\n",
    "    'audio',\n",
    "    'sub-1f9475bb-f13b-4f68-969b-28f20455b3e7_ses-CB8A74EE-0C8C-4B15-B322-D93A79ADB40A_Audio-Check_rec-Audio-Check-1.wav'\n",
    ")\n",
    "audio = Audio.from_file(str(audio_file))\n",
    "audio = audio.to_16khz()\n",
    "\n",
    "# convert to uint32 - probably should use the bits_per_sample from the original metadata!\n",
    "signal = audio.signal.squeeze()\n",
    "signal = (np.iinfo(np.uint32).max * (signal - signal.min())) / (signal.max() - signal.min())\n",
    "# display a widget to play the audio file\n",
    "Ipd.display(Ipd.Audio(data=signal, rate=audio.sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate a spectrogram of the data to visualize the frequency components over time. A spectrogram is essentially a frequency spectrum repeated `N` times spaced out throughout the original audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_length = 30\n",
    "hop_length = 10\n",
    "nfft = 512\n",
    "features_specgram = specgram(audio, win_length=win_length, hop_length=hop_length, n_fft=nfft)\n",
    "features_specgram = features_specgram.numpy()\n",
    "# convert to db\n",
    "log_spec = 10.0 * np.log10(np.maximum(features_specgram, 1e-10)).T\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_ylabel('Frequency (Hz)')\n",
    "ax.matshow(log_spec, origin=\"lower\", aspect=\"auto\")\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "xticks = ax.get_xticks()\n",
    "xticklabels = [f\"{int(t * hop_length / 1000)}\" for t in xticks]\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(xticklabels)\n",
    "# reset the xlim, which may have been modified by setting the xticks\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_xlabel('Time (s)')\n",
    "\n",
    "# y-axis is frequency\n",
    "ylim = ax.get_ylim()\n",
    "yticks = ax.get_yticks()\n",
    "# convert yticks into frequencies\n",
    "frequencies = yticks / nfft * audio.sample_rate\n",
    "frequencies = [f\"{int(f)}\" for f in frequencies]\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(frequencies)\n",
    "ax.set_ylim(ylim)\n",
    "\n",
    "# Display the image\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "b2ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
